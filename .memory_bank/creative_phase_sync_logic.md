# Creative Phase: Synchronization Logic (T08/T09)

ðŸŽ¨ðŸŽ¨ðŸŽ¨ ENTERING CREATIVE PHASE: ALGORITHM DESIGN ðŸŽ¨ðŸŽ¨ðŸŽ¨

## Component Description

This component is responsible for accurately synchronizing the audio playback generated by the Kokoro-82M TTS with the visual highlighting of corresponding words and sentences on the web page.

## Requirements & Constraints

1.  **Accuracy:** Highlighting should match the spoken audio with minimal perceptible lag or drift.
2.  **Granularity:** Must support both word-level and sentence-level highlighting.
3.  **Performance:** Must not significantly degrade browser performance, even on longer pages.
4.  **Robustness:** Should handle variations in page structure and potential inconsistencies in text segmentation or audio timing.
5.  **Input Source:** Needs to work with audio playback (likely HTML5 `<audio>` or Web Audio API) and segmented text data (words/sentences with references to their DOM nodes).
6.  **Timing Data:** The primary constraint is the *availability and format* of timing information from the Kokoro-82M TTS. The design must consider scenarios where timing data is detailed, sparse, or unavailable.

## Options Analysis

Here are potential algorithmic approaches:

**Option 1: TTS Timing Data Driven (Ideal Scenario)**

*   **Description:** Assumes Kokoro-82M API provides precise start and end timestamps for each word and/or sentence in the response along with the audio.
*   **Algorithm:**
    1.  Store the timing data (e.g., `[{word: "Hello", start: 0.1, end: 0.5}, ...]`) alongside the segmented text.
    2.  During audio playback, continuously check the `audio.currentTime`.
    3.  Use the `currentTime` to find the currently spoken word/sentence by comparing it against the stored start/end timestamps.
    4.  Apply/remove highlighting CSS classes to the corresponding DOM elements.
*   **Pros:**
    *   Potentially the most accurate method if TTS timing is reliable.
    *   Relatively straightforward logic for mapping time to text.
*   **Cons:**
    *   **Highly dependent on Kokoro-82M providing accurate, granular timing data.** (This is a major unknown).
    *   API response size might increase with detailed timing.
    *   Slight inaccuracies in TTS timing can still cause drift.
*   **Complexity:** Low-Medium (if timing data is good), High (if data is bad/missing).

**Option 2: Web Audio API + Estimated Timing**

*   **Description:** Uses the Web Audio API for finer playback control and estimates word/sentence durations if precise TTS timing is unavailable.
*   **Algorithm:**
    1.  Segment text into words/sentences.
    2.  Estimate duration for each word/sentence (e.g., based on character count, average speech rate).
    3.  Load the full audio into a Web Audio API `AudioBuffer`.
    4.  Use `AudioBufferSourceNode` for playback.
    5.  Maintain an internal timer or use `requestAnimationFrame` loop.
    6.  Track the elapsed playback time via the `AudioContext.currentTime`.
    7.  Calculate the expected current word/sentence based on elapsed time and estimated durations.
    8.  Apply/remove highlighting.
*   **Pros:**
    *   Less dependent on specific TTS API features.
    *   Web Audio API offers precise playback control and timing.
*   **Cons:**
    *   Estimation of word/sentence duration is inherently inaccurate and prone to drift, especially with varying speech patterns or pauses.
    *   Requires loading the entire audio chunk into memory via `AudioBuffer`, potentially memory-intensive for long texts.
    *   More complex client-side logic for estimation and tracking.
*   **Complexity:** Medium-High.

**Option 3: Hybrid - `speechSynthesis` API Events (Browser Voices Only / Fallback)**

*   **Description:** Leverages the browser's built-in `speechSynthesis` API and its `onboundary` event (fires when reaching word/sentence boundaries). *Note: This primarily applies if using browser voices, not directly for Kokoro-82M audio playback unless Kokoro output could somehow trigger these events (unlikely).* Useful as a fallback or reference.
*   **Algorithm:**
    1.  Use `SpeechSynthesisUtterance`.
    2.  Set up an `onboundary` event listener.
    3.  The event provides `charIndex` indicating the boundary position in the original text.
    4.  Map `charIndex` back to the corresponding word/sentence DOM element.
    5.  Apply/remove highlighting.
*   **Pros:**
    *   Built-in browser feature, potentially simpler to implement *for compatible voices*.
    *   Events are pushed, reducing need for polling `currentTime`.
*   **Cons:**
    *   **Not directly applicable to playing pre-generated audio from Kokoro-82M.**
    *   `onboundary` event support and accuracy can vary significantly between browsers and even OS voices.
    *   Mapping `charIndex` robustly back to DOM elements can be tricky.
*   **Complexity:** Medium (due to mapping and browser inconsistencies).

**Option 4: Time Update Polling + Estimated Timing**

*   **Description:** Similar to Option 2 but uses the standard HTML5 `<audio>` element and polls its `currentTime` property.
*   **Algorithm:**
    1.  Segment text and estimate durations (as in Option 2).
    2.  Play audio using `<audio>` element.
    3.  Use `setInterval` or `requestAnimationFrame` to frequently check `audio.currentTime`.
    4.  Calculate the expected current word/sentence based on polled time and estimated durations.
    5.  Apply/remove highlighting.
*   **Pros:**
    *   Simpler audio playback mechanism than Web Audio API.
    *   Less dependent on TTS API timing features.
*   **Cons:**
    *   Still relies on inaccurate duration estimation, leading to drift.
    *   Polling `currentTime` can be less precise and potentially less performant than Web Audio API timing or events.
    *   `timeupdate` event frequency isn't guaranteed, might not be granular enough for rapid word highlighting.
*   **Complexity:** Medium.

## Recommended Approach

**Primary Recommendation: Option 1 (TTS Timing Data Driven), with Option 4 (Time Update Polling + Estimation) as a Fallback.**

*   **Justification:**
    *   Option 1 offers the highest potential for accuracy *if* Kokoro-82M provides the necessary data. This should be the primary investigation path during implementation (T08). Determining the exact format and granularity of Kokoro's timing output is critical.
    *   If Kokoro timing data is insufficient or unavailable, Option 4 provides a viable, albeit less accurate, alternative using the simpler HTML5 audio element. It avoids the memory overhead of Option 2 (Web Audio API with full buffer) for the MVP.
    *   Starting with HTML5 audio (Option 4 fallback) is simpler than immediately jumping to the Web Audio API (Option 2). Web Audio API can be explored later for refinement if needed.
    *   Option 3 is not directly applicable to playing external audio from Kokoro.

## Implementation Guidelines

1.  **Investigate Kokoro Timing (T08):** During TTS integration (T03), thoroughly examine the Kokoro-82M API response. Look for any word/sentence level timestamps or metadata.
2.  **Data Structure:** Design the data structure holding segmented text (sentences/words) to accommodate optional timing information (`startTime`, `endTime`).
3.  **Core Sync Logic (T09):**
    *   Create a `SyncManager` class/module.
    *   **If Kokoro timing exists (Option 1 Path):**
        *   Feed timing data into `SyncManager`.
        *   Listen to `timeupdate` events on the `<audio>` element.
        *   In the event handler, efficiently search the timing data based on `audio.currentTime` to find the current word/sentence.
        *   Trigger highlight updates via the `Highlighter` module.
    *   **If Kokoro timing is insufficient (Option 4 Fallback Path):**
        *   Implement duration estimation logic (e.g., based on characters/phonemes per second) within `SyncManager` or `TextProcessor`.
        *   Calculate cumulative start/end times for each word/sentence based on estimates.
        *   Listen to `timeupdate` events (or use `requestAnimationFrame` for higher frequency).
        *   Compare `audio.currentTime` against estimated cumulative times to find the current word/sentence.
        *   Trigger highlight updates.
4.  **Highlighting Interface:** The `SyncManager` should call functions on the `Highlighter` module (e.g., `highlighter.highlightWord(wordId)`, `highlighter.highlightSentence(sentenceId)`).
5.  **Event Listeners:** Ensure event listeners (`timeupdate`, `ended`, `play`, `pause`) are added and removed correctly to manage the sync process and prevent memory leaks.
6.  **Refinement (T10):** Plan for iteration. The estimation fallback will likely require tuning (adjusting estimated speech rate) to minimize drift. Consider adding a small offset buffer to account for timing variations.

## Verification

*   Does the approach address the need for word and sentence highlighting? YES.
*   Does it account for the uncertainty of TTS timing data? YES (by having a primary path and a fallback).
*   Is the recommended approach feasible within an MVP context? YES (Option 1 if data exists, Option 4 fallback is manageable).
*   Are performance considerations mentioned? YES (briefly, regarding polling vs. events).
*   Are potential challenges acknowledged? YES (accuracy of timing/estimation).

ðŸŽ¨ðŸŽ¨ðŸŽ¨ EXITING CREATIVE PHASE ðŸŽ¨ðŸŽ¨ðŸŽ¨ 